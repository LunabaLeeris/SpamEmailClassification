{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this about\n",
    "This is an expirement on the difference between two forms of naive bayes + laplace smoothing in text classification, namely:\n",
    "1. Multivariate Bernoulli Event Model (MVBEM)\n",
    "2. Multinomial Event Model (MNEM)\n",
    "\n",
    "### The Goal \n",
    "The goal is to apply both versions of Naive Bayes to predict whether an email text is a SPAM or HAM and measure whether one technique is better than the other and which areas do they both excell at.\n",
    "\n",
    "### Naive Bayes summary\n",
    "The Naive Bayes, when applied to text classification, assumes that the words are conditionally independent from each other and is not affect by the classification of the text. This allows us to reduce the computation requires using bayes rule. Thus giving us the following PDF\n",
    "$$\n",
    "P(y |x) =\\frac{P(y)\\prod_{i = 1}^DP(x_i|y)}{\\prod_{j=1}^KP(y =j)\\prod_{i=1}^DP(x_i|y = j)}\n",
    "$$\n",
    "\n",
    "where $D$ is the number of words to consider and $K$ is the number of classifications\n",
    "### Multivariate Bernoulli event Model\n",
    "The MVBEM aims to calculate the provability of a certain words (from the list of words we are considering) from occuring after learning that a text is SPAM or HAM. Because the classification is binary, we only need to calculate one of the provabilities to know whether the text is spam or not. This then gives us the following Provability distribution \n",
    "$$\n",
    "P(y =1|x) =\\frac{\\prod_{i = 1}^DP(x_i|y = 1)P(y = 1)}{\\prod_{i=1}^DP(x_i|y = 0)P(y = 0) + \\prod_{i=1}^DP(x_i|y = 1)P(y = 1)}\n",
    "$$\n",
    "\n",
    "Because a word can only occur or not and a classification can only be SPAM or HAM, we will use the bernoulli distribution to model each, thus its name. This then gives us the following relationships:\n",
    "$$\n",
    "P(x_i = 1) = \\phi_{i} \\\\\n",
    "P(x_i = 0) = 1 - \\phi_{i} \\\\\n",
    "P(x_i) = \\phi_i^{I\\{\\ x_i = 1\\}}(1-\\phi_i)^{1 - I\\{ x_i = 1\\}} \\\\\n",
    "P(x_i =1 |y = 1) = \\phi_{i|y = 1}^{I\\{x_i = 1 \\wedge y^{(i)} = 1\\}} \\\\\n",
    "P(x_i|y = 1) = \\phi_{i|y = 1}^{I\\{x_i = 1 \\wedge y^{(i)} = 1\\}}(1 - \\phi_{i|y = 1})^{I{\\{y^{(i)} = 1\\}} - I\\{x_i = 1 \\wedge y^{(i)} = 1\\}} \\\\ \n",
    "P(x_i|y = 0) = \\phi_{i|y = 0}^{I\\{x_i = 1 \\wedge y^{(i)} = 0\\}}(1 - \\phi_{i|y = 0})^{I\\{y^{(i)} = 0\\} - I\\{x_i = 0 \\wedge y^{(i)} = 0\\}}\n",
    "$$\n",
    "After applying these to the PDF, we can use MLE to get the best parameters. We then apply laplace smoothing to take into account zero possibilities thus giving us the following results:\n",
    "\n",
    "$$\n",
    "\\phi_y = \\frac{\\sum_{j=1}^NI\\{y^{(j)} = 1\\}}{N}\\\\\n",
    "\\phi_{i|y = 1} = \\frac{a +\\sum_{j=1}^NI\\{x_i^{(j)} = 1 \\wedge y^{(j)} = 1\\}} {2a +\\sum_{j=1}^NI\\{y^{(j)} = 1\\}} \\\\\n",
    "\\phi_{i|y = 0} = \\frac{a+ \\sum_{j=1}^NI\\{x_i^{(j)} = 1 \\wedge y^{(j)} = 0\\}}{2a +\\sum_{j=1}^NI\\{y^{(j)} = 0\\}}\n",
    "$$\n",
    "\n",
    "where $a$ is our smoothing parameter\n",
    "\n",
    "The above calculations can be simplified using matrices\n",
    "$$\n",
    "\\phi_y = \\frac{1}{N}sum(Y) \\\\\n",
    "\\phi_{i|y=1} = \\frac{YX}{sum(Y)} \\\\\n",
    "\\phi_{i|y=0} = \\frac{(1-Y)X}{N - sum(Y)}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$Y \\epsilon \\R^{1 \\times N}$ = where the $i'th$  entry represents whether the $i'th$ text is a spam or not (1, 0)\n",
    "\n",
    "$X \\epsilon\\R^{N \\times D}$ = where the $i'th$ row is a vector representing whether certain words occured in the $i'th$ text\n",
    "\n",
    "lastly to make predicitions we will use the following formula:\n",
    "\n",
    "$$\n",
    "P(x_i|y=1) = \\frac{(1-\\phi_y)prod(P_{i|y=0}^{X} \\cdot(1-P_{i|y=0})^{1-X})}{\\phi_yprod(P_{i|y=1}^{X} \\cdot(1-P_{i|y=1})^{1-X}} < 1 : SPAM\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$\\phi_y \\epsilon \\R$ \n",
    "\n",
    "$P_{i|y=0}\\epsilon\\R^{1\\times D}$ = is a row vector where the $i'th$ entry is $\\phi_{i|y=0}$ \n",
    "\n",
    "$P_{i|y=1}\\epsilon\\R^{1\\times D}$ = is a row vector where the $i'th$ entry is $\\phi_{i|y=1}$ \n",
    "\n",
    "$X \\epsilon \\R^{1 \\times D}$ = is the feature vector to predict where the $i'th$ entry represents the existance of the $i'th$  word in the text (either 1 or 0) \n",
    "\n",
    "$D$ = denotes the number of words to consider\n",
    "### Multinomial Event Model\n",
    "Unlike the MVBEM, the MNEM calculates that provability that a certain word will occur at a certain position in the text. This allows the model to take into account repeating words, which is ignored by the previous model. Because every position can have $D$ possitibilies, we will use the multi-nomial model to model this provability. This then gives us the following PDF\n",
    "\n",
    "$$\n",
    "P(y =1|x) =\\frac{\\prod_{i = 1}^LP(x_i|y = 1)P(y = 1)}{\\prod_{i=1}^LP(x_i|y = 0)P(y = 0) + \\prod_{i=1}^LP(x_i|y = 1)P(y = 1)}\n",
    "$$\n",
    "\n",
    "where $L$ is the length of the text. We will then use the following models\n",
    "$$\n",
    "P(y) = \\phi_y^{I\\{y^{(i)} = 1\\}}(1 - \\phi_y)^{1 - {I\\{y^{(i)} = 1\\}}} \\\\\n",
    "P(x_k) = \\frac{\\prod_{i=1}^{D}\\phi_i^{I\\{x_k = i\\}}}{\\sum_{i=1}^D\\phi_i} \\\\\n",
    "P(x_k|y=1) = \\frac{\\prod_{i=1}^{D}\\phi_i^{I\\{x_k = i \\wedge y^{(k) = 1}\\}}}{\\sum_{i=1}^D\\phi_i^{I\\{ y^{(k) = 1}\\}}} \\\\ \n",
    "P(x_k|y=0) = \\frac{\\prod_{i=1}^{D}\\phi_i^{I\\{x_k = i \\wedge y^{(k)} = 0\\}}}{\\sum_{i=1}^D\\phi_i^{I\\{y^{(k) = 0}\\}}}P(y) = \\phi_y^{I\\{y^{(i)} = 1\\}}(1 - \\phi_y)^{1 - {I\\{y^{(i)} = 1\\}}} \\\\\n",
    "P(x_k) = \\frac{\\prod_{i=1}^{D}\\phi_i^{I\\{x_k = i\\}}}{\\sum_{i=1}^D\\phi_i} \\\\\n",
    "P(x_k|y=1) = \\frac{\\prod_{i=1}^{D}\\phi_i^{I\\{x_k = i \\wedge y^{(k) = 1}\\}}}{\\sum_{i=1}^D\\phi_i^{I\\{ y^{(k) = 1}\\}}} \\\\ \n",
    "P(x_k|y=0) = \\frac{\\prod_{i=1}^{D}\\phi_i^{I\\{x_k = i \\wedge y^{(k)} = 0\\}}}{\\sum_{i=1}^D\\phi_i^{I\\{y^{(k) = 0}\\}}}\n",
    "$$\n",
    "\n",
    "And it turns out if we compute for each parameter, in addition with laplace smoothing, we will have\n",
    "$$\n",
    "\\phi_y = \\frac{\\sum_{i = 1}^NI\\{y^{(i)} = 1\\}}{N} \\\\\n",
    "\\phi_{k|y=1}= \\frac{1 + \\sum_{i = 1}^NI\\{y^{(i)} = 1\\}\\sum_{j=1}^DI\\{\\ x_j^{(i)}=k\\}}{D + \\sum_{i = 1}^NI\\{y^{(i)} = 1\\}L_i} \\\\\n",
    "\\phi_{k|y=0}= \\frac{1 + \\sum_{i = 1}^NI\\{y^{(i)} = 0\\}\\sum_{j=1}^DI\\{\\ x_j^{(i)}=k\\}}{D + \\sum_{i = 1}^NI\\{y^{(i)} = 0\\}L_i}\n",
    "$$\n",
    "\n",
    "where $\\phi_{k|y=1}$ is the provability that a certain word will occur at a certain possition given that the text is a SPAM. Notice that there is not indicator of the position this is because the provability that a ceratain word will occur is not affected by the position, this is also one of the major drawbacks of this approach as position is vital in texts as words follow a certain gramatical structure\n",
    "\n",
    "Lastly to make predictions, we will use the following formula\n",
    "$$\n",
    "\\frac{(1 - \\phi_y)\\sum_{i=1}^D\\phi_{i|y=1}}{\\phi_y\\sum_{i=1}^D\\phi_{i|y=0}} \\prod_{k=1}^L\\frac{\\prod_{i=1}^{D}\\phi_{i|y=0}^{I\\{x_k = i \\}}}{\\prod_{i=1}^{D}\\phi_{i|y=1}^{I\\{x_k = i \\}}} < 1 : spam\n",
    "$$\n",
    "\n",
    "### Training Techniques\n",
    "Notice that in order for both Naive Bayes to work, we have to consider some set of $D$ words. One way to do this is to just consider the top $K$ words in the dictionary. But this may be bad especially given the fact the new words come up each day. A solution to this is to only consider the words that appeared in the training set. This is the tecnique we will use. The problem with this is that the number of words to consider can balloon exponentially thus exceeding the RAM (this happened to my 24 gb ram laptop). To compensate, a good approach will be to start with the top K words in dictionary then add new words with a certain limit. Once we exceed that limit, we stop adding new words\n",
    "\n",
    "Also, notice that for multi-nomial event models, when predicting, we have to take into account the fact that the word we are processing may not exist in our set of words $D$. One way to solve this is to assign one number to a token that signifies a word does not exist. Letâ€™s say our words are of length 10,000, we can denote 10,001 as a representation of words that does not exist.\n",
    "\n",
    "Also, all words are lowercased to take into account similar words with different capitalization. I am aware that the capitalization of words also contribute to determining whether a text is SPAM or HAM, but to lessen the amount of parameters (words) to consider, I will take this gamble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Bernoulli Event Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: DATA PROCESSING\n",
    "First we have to process the data into our intended format. Specifically to \n",
    "\n",
    "$Y \\epsilon \\R^{1 \\times N}$ = where the $i'th$  entry represents whether the $i'th$ text is a spam or not (1, 0)\n",
    "\n",
    "$X \\epsilon\\R^{N \\times D}$ = where the $i'th$ row is a vector representing whether certain words occured in the $i'th$ text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (2809127371.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 31\u001b[1;36m\u001b[0m\n\u001b[1;33m    y_chunk = np.array(chunk[:, label_col_num])                                                                                               ])\u001b[0m\n\u001b[1;37m                                                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "# Configurations\n",
    "dataset_path1 = \"dataset/spam_ham_dataset.csv\"\n",
    "dataset_path2 = \"dataset/spam.csv\"\n",
    "chunksize = 1000                            # Adjust depending on your RAM capacity\n",
    "words_to_consider_limit = 10_000            # Adjust depending on your RAM capacity. this is also same with D \n",
    "len_words_to_skip = 15                      # If words exceed this limit, we ignore it\n",
    "classifications = {\"spam\": 1, \"ham\": 0}     # change accordingly\n",
    "\n",
    "# The convert to int parameter is used for data where the labeling is spam=1 and ham=0 instead of just 1 and 0\n",
    "def parse_data(chunksize, path, text_col_num, label_col_num, encoding=\"UTF-8\", convert_to_int=False):\n",
    "    print(\"Parsing data\")\n",
    "    df = pd.read_csv(path, encoding=encoding, chunksize=chunksize)\n",
    "\n",
    "    # Things to return\n",
    "    words_to_consider = {}\n",
    "    Y = None\n",
    "    X = None\n",
    "\n",
    "    chunks_processed = 0\n",
    "    for chunk in df:\n",
    "        chunk = chunk.to_numpy()\n",
    "\n",
    "        if convert_to_int:\n",
    "            y_chunk = np.array([[classifications[c] for c in chunk[:, label_col_num]]])\n",
    "        else:\n",
    "            y_chunk = np.array(chunk[:, label_col_num])\n",
    "        \n",
    "        if chunks_processed == 0:                                   \n",
    "            Y = tf.constant(y_chunk, dtype=tf.uint8)                                                                                            \n",
    "        else:\n",
    "            Y = tf.concat((Y, y_chunk), axis=0)\n",
    "        \n",
    "        # Parse each text to a matrix\n",
    "        for i in range(len(chunk)):\n",
    "            print(f\"Processing {chunks_processed * chunksize + i}\")\n",
    "            text = chunk[i, text_col_num]\n",
    "            matrix = np.array([map_text_to_matrix(text, words_to_consider)])                                                                                                                                                                                       \n",
    "\n",
    "            if chunks_processed == 0 and i == 0:\n",
    "                X = tf.constant(matrix, dtype=tf.uint8)                                                \n",
    "            else:                           \n",
    "                X = tf.concat((X, matrix), axis=0)\n",
    "\n",
    "            del text, matrix\n",
    "            print(f\"Successfully processed {chunks_processed * chunksize + i}\")\n",
    "\n",
    "        # Manual deletion to save RAM\n",
    "        chunks_processed += 1\n",
    "        del chunk, y_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    # Adjust size of every row in x_list to be D\n",
    "    D = len(words_to_consider)\n",
    "    N = len(X)\n",
    "    print(\"Successfully parsed data\")\n",
    "    return X, Y, N, D, words_to_consider\n",
    "\n",
    "def map_text_to_matrix(text, words_to_consider):\n",
    "    D = len(words_to_consider)\n",
    "    matrix = np.zeros(words_to_consider_limit)\n",
    "\n",
    "    for word in text.split():\n",
    "        word = word.lower()\n",
    "        if word not in words_to_consider:\n",
    "            if D >= words_to_consider_limit:\n",
    "                continue\n",
    "            \n",
    "            if len(word) >= len_words_to_skip:\n",
    "                continue\n",
    "\n",
    "            words_to_consider[word] = D\n",
    "            D += 1\n",
    "        \n",
    "        matrix[words_to_consider[word]] = 1\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def pad_to_size(arr, size):\n",
    "    # Assumes that size >= len(arr)\n",
    "    return np.pad(arr, (0, size - len(arr)), mode='constant')\n",
    "\n",
    "def display_tensor(tensor, name):\n",
    "    print(f\"{name}: \\n\", tensor.numpy())\n",
    "\n",
    "# Parse the data\n",
    "X, Y, N, D, words_to_consider = parse_data(chunksize, dataset_path2, 2, 3, \"latin-1\", True)\n",
    "\n",
    "print(\"Total number of words to consider: \", D)\n",
    "print(\"Total number of emails: \", N)                \n",
    "print(\"Words to consider: \", words_to_consider)                  \n",
    "display_tensor(X, \"X\")                                                                                                                                                                                                                                                 \n",
    "display_tensor(Y, \"Y\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
